# <center>HiGPT: Heterogeneous Graph Language Model</center>

[Jiabin Tang](https://tjb-tech.github.io/), [Yuhao Yang](http://yuh-yang.github.io), [Wei Wei](#), [Lei Shi](#), [Long Xia](#), [Dawei Yin](https://www.yindawei.com/) and [Chao Huang](https://sites.google.com/view/chaoh/home)*.
(*Correspondence )

**[Data Intelligence Lab](https://sites.google.com/view/chaoh/home)@[University of Hong Kong](https://www.hku.hk/)**, Baidu Inc.

-----

<a href='https://HiGPT-HKU.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>
<a href='#'><img src='https://img.shields.io/badge/Demo-Page-purple'></a> 
<a href='#'><img src='https://img.shields.io/badge/Paper-PDF-orange'></a> 
[![YouTube](https://badges.aleen42.com/src/youtube.svg)](#)
 â€¢ ğŸŒ <a href="https://mp.weixin.qq.com/s/rvKTFdCk719Q6hT09Caglw" target="_blank">ä¸­æ–‡åšå®¢</a>


This repository hosts the code, data and model weight of **HiGPT**.

-----------

## ğŸ‰ News 


ğŸ¯ğŸ¯ğŸ“¢ğŸ“¢ We have made significant updates to the **models** used in our HiGPT on ğŸ¤— **Huggingface**. We highly recommend referring to the table below for further details: 

| ğŸ¤— Huggingface Address                   | ğŸ¯ Description |
| --------------------------------------- | ------------- |
| https://huggingface.co/Jiabin99/MetaHGT |               |
|                                         |               |


- [x] [2023.10.26]ğŸ”¥ğŸ”¥Release our utilized Instruction data.
- [x] [2023.10.26]ğŸ”¥ğŸ”¥Release checkpoints of our HiGPT and pre-trained graph encoder.
- [x] [2023.10.15] ğŸš€ğŸš€ Release the code of HiGPT.


## ğŸ‘‰ TODO 
- [ ] Supporting lightning training
- [ ] Releasing the Chinese version of the explanation
- [ ] Releasing the full paper of our HiGPT
- [ ] Exploring the potential of our HiGPT for more graph learning tasks.
- [ ] ...

-----------




<span id='introduction'/>

## Brief Introduction 

we present the **HiGPT** framework that aligns LLMs with heterogeneous graph structural knowledge with a heterogeneous graph instruction tuning paradigm.




For more technical details, kindly refer to the [paper](#) and the project [website](https://HiGPT-HKU.github.io/) of our Graph. 


-----------

<span id='Usage'/>

## Getting Started

<span id='all_catelogue'/>

### Table of Contents:
* <a href='#Code Structure'>1. Code Structure</a>
* <a href='#Environment Preparation'>2. Environment Preparation </a>
* <a href='#Training HiGPT'>3. Data PreparationÂ </a>
* <a href='#Training HiGPT'>4. Training HiGPT </a>
  * <a href='#Prepare Pre-trained Checkpoint'>4.1. Prepare Pre-trained Checkpoint</a>
  * <a href='#Self-Supervised Instruction Tuning'>4.2. Self-Supervised Instruction Tuning</a>
  * <a href='#Extract the Trained Projector'>4.3. Extract the Trained Projector</a>
  * <a href='#Task-Specific Instruction Tuning'>4.4. Task-Specific Instruction Tuning</a>
* <a href='#Evaluating HiGPT'>5. Evaluating HiGPT</a>
  * <a href='#Preparing Checkpoints and Data'>5.1. Preparing Checkpoints and Data</a>
  * <a href='#Running Evaluation'>5.2. Running Evaluation</a>

****



<span id='Code Structure'/>

### 1. Code Structure <a href='#all_catelogue'>[Back to Top]</a>

```
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ base_model.py
â”œâ”€â”€ dist_utils.py
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ arena.md
â”‚   â”œâ”€â”€ commands
â”‚   â”‚   â”œâ”€â”€ data_cleaning.md
â”‚   â”‚   â”œâ”€â”€ leaderboard.md
â”‚   â”‚   â”œâ”€â”€ local_cluster.md
â”‚   â”‚   â”œâ”€â”€ pypi.md
â”‚   â”‚   â””â”€â”€ webserver.md
â”‚   â”œâ”€â”€ langchain_integration.md
â”‚   â”œâ”€â”€ openai_api.md
â”‚   â”œâ”€â”€ server_arch.md
â”‚   â”œâ”€â”€ test_process.md
â”‚   â”œâ”€â”€ vicuna_weights_version.md
â”‚   â””â”€â”€ weights_version.md
â”œâ”€â”€ examples
â”‚   â””â”€â”€ langchain
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ chatgpt_clone.ipynb
â”‚       â”œâ”€â”€ qa.ipynb
â”‚       â””â”€â”€ twitter_algo_analysis.ipynb
â”œâ”€â”€ hi_datasets
â”‚   â”œâ”€â”€ get_stage1_data.sh
â”‚   â””â”€â”€ get_stage2_data.sh
â”œâ”€â”€ higpt
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-38.pyc
â”‚   â”‚   â””â”€â”€ conversation.cpython-38.pyc
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ conversation.py
â”‚   â”œâ”€â”€ eval
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ run_higpt.py
â”‚   â”‚   â”œâ”€â”€ run_higpt_incontext.py
â”‚   â”‚   â””â”€â”€ webpage
â”‚   â”‚       â”œâ”€â”€ figures
â”‚   â”‚       â”‚   â”œâ”€â”€ alpaca.png
â”‚   â”‚       â”‚   â”œâ”€â”€ bard.jpg
â”‚   â”‚       â”‚   â”œâ”€â”€ chatgpt.svg
â”‚   â”‚       â”‚   â”œâ”€â”€ llama.jpg
â”‚   â”‚       â”‚   â”œâ”€â”€ swords_FILL0_wght300_GRAD0_opsz48.svg
â”‚   â”‚       â”‚   â””â”€â”€ vicuna.jpeg
â”‚   â”‚       â”œâ”€â”€ index.html
â”‚   â”‚       â”œâ”€â”€ script.js
â”‚   â”‚       â””â”€â”€ styles.css
â”‚   â”œâ”€â”€ model
â”‚   â”‚   â”œâ”€â”€ GraphLlama.py
â”‚   â”‚   â”œâ”€â”€ GraphLlama_pl.py
â”‚   â”‚   â”œâ”€â”€ HeteroLlama.py
â”‚   â”‚   â”œâ”€â”€ HeteroLlama_pl.py
â”‚   â”‚   â”œâ”€â”€ MetaHGTConv_pl.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”œâ”€â”€ GraphLlama.cpython-38.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ HeteroLlama.cpython-38.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.cpython-38.pyc
â”‚   â”‚   â”‚   â””â”€â”€ model_adapter.cpython-38.pyc
â”‚   â”‚   â”œâ”€â”€ apply_delta.py
â”‚   â”‚   â”œâ”€â”€ apply_lora.py
â”‚   â”‚   â”œâ”€â”€ builder.py
â”‚   â”‚   â”œâ”€â”€ chatglm_model.py
â”‚   â”‚   â”œâ”€â”€ compression.py
â”‚   â”‚   â”œâ”€â”€ convert_fp16.py
â”‚   â”‚   â”œâ”€â”€ graph_layers
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clip_graph.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ graph_transformer.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mpnn.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ simple_tokenizer.cpython-38.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ bpe_simple_vocab_16e6.txt.gz
â”‚   â”‚   â”‚   â”œâ”€â”€ clip_graph.py
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_transformer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ mpnn.py
â”‚   â”‚   â”‚   â””â”€â”€ simple_tokenizer.py
â”‚   â”‚   â”œâ”€â”€ heteclip_models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clip_outputs.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pretrained.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ transform.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ utils.cpython-38.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ bpe_simple_vocab_16e6.txt.gz
â”‚   â”‚   â”‚   â”œâ”€â”€ clip_outputs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ loss.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pics
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ CLIP.png
â”‚   â”‚   â”‚   â”œâ”€â”€ pretrained.py
â”‚   â”‚   â”‚   â”œâ”€â”€ timm_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚   â”œâ”€â”€ make_delta.py
â”‚   â”‚   â”œâ”€â”€ meta_hgt
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hgt_constants.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ meta_hgtconv.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ meta_hgtconv_bert_all.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ meta_linear.cpython-38.pyc
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ tokenizer.cpython-38.pyc
â”‚   â”‚   â”‚   â”œâ”€â”€ bpe_simple_vocab_16e6.txt.gz
â”‚   â”‚   â”‚   â”œâ”€â”€ hgt_constants.py
â”‚   â”‚   â”‚   â”œâ”€â”€ meta_dict
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ acm
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ edge_type.pt
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ node_type.pt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dblp
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ edge_type.pt
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ node_type.pt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ imdb
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ edge_type.pt
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ node_type.pt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ to_tensor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ meta_hgtconv.py
â”‚   â”‚   â”‚   â”œâ”€â”€ meta_hgtconv_bert_all.py
â”‚   â”‚   â”‚   â”œâ”€â”€ meta_linear.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ori_hgt.py
â”‚   â”‚   â”‚   â””â”€â”€ tokenizer.py
â”‚   â”‚   â”œâ”€â”€ model_adapter.py
â”‚   â”‚   â”œâ”€â”€ model_registry.py
â”‚   â”‚   â”œâ”€â”€ monkey_patch_non_inplace.py
â”‚   â”‚   â”œâ”€â”€ rwkv_model.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ protocol
â”‚   â”‚   â””â”€â”€ openai_api_protocol.py
â”‚   â”œâ”€â”€ serve
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api_provider.py
â”‚   â”‚   â”œâ”€â”€ bard_worker.py
â”‚   â”‚   â”œâ”€â”€ cacheflow_worker.py
â”‚   â”‚   â”œâ”€â”€ cli.py
â”‚   â”‚   â”œâ”€â”€ controller.py
â”‚   â”‚   â”œâ”€â”€ gateway
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â””â”€â”€ nginx.conf
â”‚   â”‚   â”œâ”€â”€ gradio_block_arena_anony.py
â”‚   â”‚   â”œâ”€â”€ gradio_block_arena_named.py
â”‚   â”‚   â”œâ”€â”€ gradio_css.py
â”‚   â”‚   â”œâ”€â”€ gradio_patch.py
â”‚   â”‚   â”œâ”€â”€ gradio_web_server.py
â”‚   â”‚   â”œâ”€â”€ gradio_web_server_multi.py
â”‚   â”‚   â”œâ”€â”€ huggingface_api.py
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”œâ”€â”€ model_worker.py
â”‚   â”‚   â”œâ”€â”€ monitor
â”‚   â”‚   â”‚   â”œâ”€â”€ basic_stats.py
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_battle_data.py
â”‚   â”‚   â”‚   â”œâ”€â”€ elo_analysis.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hf_space_leaderboard_app.py
â”‚   â”‚   â”‚   â””â”€â”€ monitor.py
â”‚   â”‚   â”œâ”€â”€ openai_api_server.py
â”‚   â”‚   â”œâ”€â”€ register_worker.py
â”‚   â”‚   â”œâ”€â”€ test_message.py
â”‚   â”‚   â””â”€â”€ test_throughput.py
â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â””â”€â”€ graphchat_trainer.cpython-38.pyc
â”‚   â”‚   â”œâ”€â”€ graphchat_trainer.py
â”‚   â”‚   â”œâ”€â”€ llama_flash_attn_monkey_patch.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ train_flant5.py
â”‚   â”‚   â”œâ”€â”€ train_g.py
â”‚   â”‚   â”œâ”€â”€ train_graph.py
â”‚   â”‚   â”œâ”€â”€ train_graph_back.py
â”‚   â”‚   â”œâ”€â”€ train_hete.py
â”‚   â”‚   â”œâ”€â”€ train_hete_nopl.py
â”‚   â”‚   â”œâ”€â”€ train_hete_nopl_back_2_5.py
â”‚   â”‚   â”œâ”€â”€ train_hete_nopl_wo_IA.py
â”‚   â”‚   â”œâ”€â”€ train_hete_nopl_wo_graph.py
â”‚   â”‚   â”œâ”€â”€ train_hete_old.py
â”‚   â”‚   â”œâ”€â”€ train_light.py
â”‚   â”‚   â”œâ”€â”€ train_llava.py
â”‚   â”‚   â”œâ”€â”€ train_lora.py
â”‚   â”‚   â””â”€â”€ train_mem.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ playground
â”‚   â”œâ”€â”€ inspect_conv.py
â”‚   â”œâ”€â”€ test_embedding
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ test_classification.py
â”‚   â”‚   â”œâ”€â”€ test_semantic_search.py
â”‚   â”‚   â””â”€â”€ test_sentence_similarity.py
â”‚   â””â”€â”€ test_openai_api
â”‚       â”œâ”€â”€ anthropic_api.py
â”‚       â””â”€â”€ openai_api.py
â”œâ”€â”€ run_offline_hgt_tokenizer.py
â”œâ”€â”€ run_offline_hgt_tokenizer_single.py
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ eval_script
â”‚   â”‚   â”œâ”€â”€ cal_acc_imdb_metric.py
â”‚   â”‚   â”œâ”€â”€ hetegpt_info_imdb_cot_incontext.sh
â”‚   â”‚   â””â”€â”€ higpt_info_imdb_cot.sh
â”‚   â”œâ”€â”€ extract_graph_projector.py
â”‚   â”œâ”€â”€ serving
â”‚   â”‚   â”œâ”€â”€ controller.yaml
â”‚   â”‚   â””â”€â”€ model_worker.yaml
â”‚   â””â”€â”€ tune_script
â”‚       â”œâ”€â”€ extract_projector.sh
â”‚       â”œâ”€â”€ higpt_stage_1.sh
â”‚       â”œâ”€â”€ higpt_stage_2.sh
â”‚       â”œâ”€â”€ run_graph_tokenizer.sh
â”‚       â””â”€â”€ run_graph_tokenizer_single.sh
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ test_openai_curl.sh
â”‚   â”œâ”€â”€ test_openai_langchain.py
â”‚   â””â”€â”€ test_openai_sdk.py
â””â”€â”€ utils.py
```


<span id='Environment Preparation'/>


### 2. Environment Preparation  <a href='#all_catelogue'>[Back to Top]</a>
Please first clone the repo and install the required environment, which can be done by running the following commands:
```shell
conda create -n graphgpt python=3.8

conda activate higpt

# Torch with CUDA 11.7
pip install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117
# To support vicuna base model
pip3 install "fschat[model_worker,webui]"
# To install pyg and pyg-relevant packages
pip install torch_geometric
pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-1.13.0+cu117.html
# Clone our HiGPT
git clone https://github.com/HKUDS/HiGPT.git
cd HiGPT
# Install required libraries
pip install -r requirements.txt
```

<span id='Training HiGPT'/>

### 4. Training HiGPT <a href='#all_catelogue'>[Back to Top]</a>

HiGPT tuning paradigm consists of two stages: (1) self-supervised instruction tuning; (2) task-specific instruction tuning.

<span id='Prepare Pre-trained Checkpoint'/>

#### 4.1. Preparing Pre-trained Checkpoint  <a href='#all_catelogue'>[Back to Top]</a>
HiGPT is trained based on following excellent existing models.
Please follow the instructions to prepare the checkpoints.

- `Vicuna`:
  Prepare our base model Vicuna, which is an instruction-tuned chatbot and base model in our implementation. Please download its weights [here](https://github.com/lm-sys/FastChat#model-weights). We generally utilize v1.1 and v1.5 model with 7B parameters.

- `Graph Encoder`:
  is used to encode graph structures. We employ text-graph grounding approach to obtain the pre-trained graph transformer model, which you could download by [graph transformer](https://huggingface.co/Jiabin99/Arxiv-PubMed-GraphCLIP-GT) and put it at [[./HiGPT]](./HiGPT). We also provide source codes and example Cora data for text-graph grounding at [[./text-graph-grounding]](./text-graph-grounding) for your reference.

- `Graph Data`:
  is a combination of all utilized pyg graph data that contain node features, edge_index and so on. You can download by [all_graph_data.pt](https://huggingface.co/datasets/Jiabin99/All_pyg_graph_data) and put it at [[./HiGPT/graph_data]](./HiGPT/graph_data)

<span id='Self-Supervised Instruction Tuning'/>

#### 4.2. Self-Supervised Instruction Tuning  <a href='#all_catelogue'>[Back to Top]</a>

* **Prepare data:** Please download our instruction tuning data [graph_matching.json](https://huggingface.co/datasets/Jiabin99/graph-matching) for the graph matching task.

* **Start tuning:** After the aforementioned steps, you could start the first stage tuning by filling blanks at [graphgpt_stage1.sh](scripts/tune_script/graphgpt_stage1.sh). There is an example as below: 

```shell
# to fill in the following path to run the first stage of our HiGPT!
model_path=../vicuna-7b-v1.5-16k
instruct_ds=./data/stage_1/graph_matching.json
graph_data_path=./graph_data/all_graph_data.pt
pretra_gnn=./clip_gt_arxiv
output_model=./checkpoints/stage_1

wandb offline
python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 --master_port=20001 \
    graphgpt/train/train_mem.py \
    --model_name_or_path ${model_path} \
    --version v1 \
    --data_path ${instruct_ds} \
    --graph_content ./arxiv_ti_ab.json \
    --graph_data_path ${graph_data_path} \
    --graph_tower ${pretra_gnn} \
    --tune_graph_mlp_adapter True \
    --graph_select_layer -2 \
    --use_graph_start_end \
    --bf16 True \
    --output_dir ${output_model} \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2400 \
    --save_total_limit 1 \
    --learning_rate 2e-3 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --lazy_preprocess True \
    --report_to wandb
```

<span id='Extract the Trained Projector'/>

#### 4.3. Extract the Trained Projector  <a href='#all_catelogue'>[Back to Top]</a>

We could extract the trained projector in the stage 1 by filling blanks at [extract_projector.sh](scripts/tune_script/extract_projector.sh). There is an example as below: 

```shell
# to fill in the following path to extract projector for the first tuning stage!
src_model=./checkpoints/stage_1
output_proj=./checkpoints/stage_1_projector/stage_1_projector.bin

python3.8 ./scripts/extract_graph_projector.py \
  --model_name_or_path ${src_model} \
  --output ${output_proj}
```

<span id='Task-Specific Instruction Tuning'/>

#### 4.4. Task-Specific Instruction Tuning  <a href='#all_catelogue'>[Back to Top]</a>

* **Prepare data:** The choices of our task-specific instruction data could be diverse, e.g., standard or COT (Chain-of-Thought) node classification, link prediction or mixing data for multitasking. Please refer to the  [task_specific](https://huggingface.co/datasets/Jiabin99/Arxiv-PubMed-mix-NC-LP).

* **Start tuning:** After the aforementioned steps, you could start the second stage tuning by filling blanks at [graphgpt_stage2.sh](scripts/tune_script/graphgpt_stage2.sh). There is an example as below: 

```shell
# to fill in the following path to run the second stage of our HiGPT!
model_path=../vicuna-7b-v1.5-16k
instruct_ds=./data/stage_2/data_all_mix.json
graph_data_path=./graph_data/all_graph_data.pt
pretra_gnn=./clip_gt_arxiv
tuned_proj=./checkpoints/stage_1_projector/stage_1_projector.bin
output_model=./checkpoints/stage_2

wandb offline
python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 --master_port=20001 \
    graphgpt/train/train_mem.py \
    --model_name_or_path ${model_path} \
    --version v1 \
    --data_path ${instruct_ds} \
    --graph_content ./arxiv_ti_ab.json \
    --graph_data_path ${graph_data_path} \
    --graph_tower ${pretra_gnn} \
    --pretrain_graph_mlp_adapter ${tuned_proj} \
    --tune_graph_mlp_adapter True \
    --graph_select_layer -2 \
    --use_graph_start_end True\
    --bf16 True \
    --output_dir ${output_model} \
    --num_train_epochs 2 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 50000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --lazy_preprocess True \
    --report_to wandb

```



<span id='Evaluating HiGPT'/>

## 5. Evaluating HiGPT  <a href='#all_catelogue'>[Back to Top]</a>

<span id='Preparing Checkpoints and Data'/>


#### 5.1. Preparing Checkpoints and Data <a href='#all_catelogue'>[Back to Top]</a>

* **Checkpoints:** You could try to evaluate HiGPT by using your own model or our released checkpoints.
* **Data:** We split test sets for different graph datasets and make the instruction data for evaluation. Please refer to the  [evaluating](https://huggingface.co/datasets/Jiabin99/HiGPT-eval-instruction).

<span id='Running Evaluation'/>

#### 5.2. Running Evaluation <a href='#all_catelogue'>[Back to Top]</a>

You could start the second stage tuning by filling blanks at [graphgpt_eval.sh](scripts/eval_script/graphgpt_eval.sh). There is an example as below: 
```shell
# to fill in the following path to extract projector for the second tuning stage!
output_model=./checkpoints/stage_2
datapath=./data/eval/arxiv_nc.json
graph_data_path=./graph_data/all_graph_data.pt
res_path=./output_stage_2_arxiv_nc
start_id=0
end_id=20000
num_gpus=2

python3.8 ./graphgpt/eval/run_graphgpt.py --model-name ${output_model}  --prompting_file ${datapath} --graph_data_path ${graph_data_path} --output_res_path ${res_path} --start_id ${start_id} --end_id ${end_id} --num_gpus ${num_gpus}
```
---------


## Contact

For any questions or feedback, feel free to contact [Jiabin Tang](mailto:jiabintang77@gmail.com).


## Citation

If you find HiGPT useful in your research or applications, please kindly cite:
```tex
@articles{tang2023graphgpt,
title={HiGPT: Graph Instruction Tuning for Large Language Models}, 
author={Jiabin Tang and Yuhao Yang and Wei Wei and Lei Shi and Long Xia and Dawei Yin and Chao Huang},
year={2024},
eprint={},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
```



## Acknowledgements
You may refer to related work that serves as foundations for our framework and code repository, 
[Vicuna](https://github.com/lm-sys/FastChat), [LLaVa](https://github.com/haotian-liu/LLaVA), [GraphGPT](https://github.com/HKUDS/GraphGPT), We also partially draw inspirations from [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4). The design of our website and README.md was inspired by [NExT-GPT](https://next-gpt.github.io/). Thanks for their wonderful works.



